---
title: "Final Project"
author: "Harrison Marick, Meredith Manley, Christien Wright"
date: "December 22, 2017"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: false
    df_print: kable
---




## Load all packages

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rpart)
library(rpart.plot)
library(class)
library(MLmetrics)
library(tidyr)
library(dplyr)
library(grid)
library(jpeg)
library(rjson)
library(RCurl)
library(caret)
library(densityvis)
library(ggplot2)
library(glmnet)
```



## Load data and perform data cleaning

Please delete all "notes" before submission.

Note: CSV/data files should be read assuming they are in the `data` folder. In
other words, load data via `read_csv("data/CSV_NAME.csv")` and not via
`read_csv("/Users/aykim/Documents/MATH495/Final_Project/data/CSV_NAME.csv")`

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
shots<-read.csv("data/data.csv") #read in data
test<-subset(shots, is.na(shot_made_flag))
train<-subset(shots, !is.na(shot_made_flag))

# convert all characters to factors in train and test set
train <- train %>%
  mutate(season = as.factor(season),
         shot_type = as.factor(shot_type),
         shot_zone_area = as.factor(shot_zone_area),
         shot_zone_basic = as.factor(shot_zone_basic),
         shot_zone_range = as.factor(shot_zone_range),
         team_name = as.factor(team_name),
         matchup = as.factor(matchup),
         opponent = as.factor(opponent))

test <- test %>%
  mutate(season = as.factor(season),
         shot_type = as.factor(shot_type),
         shot_zone_area = as.factor(shot_zone_area),
         shot_zone_basic = as.factor(shot_zone_basic),
         shot_zone_range = as.factor(shot_zone_range),
         team_name = as.factor(team_name),
         matchup = as.factor(matchup),
         opponent = as.factor(opponent))
```



## EDA visualizations and tables

Below, we have a heat map that has broken the court into hexagons with which to group shots. The size of the points correspond to the volume of shots in a given hexagon, and the color corresponds to the accuracy.

Notice in the above plot that the largest point is by far the one directly underneath the basket. Kobe Bryant made a career out of getting to the basket, and when he got there, he rarely missed. As expected, the points tend to turn blue and dark purple the further from the basket we move. Bryant's efficiency, as expected, decreased, as he moved towards the three point arc and beyond. Outside the point closest to the hoop, Bryant's high volume regions tend to be on the wings, both in the mid range and at the three point arc. It is clear that both location and distance play a role in the likelihood a shot is made. Having said that, obviously the two variables are not completely independent of one another, so the inclusion of both in our models was unnecessary.

```{r, echo=FALSE}
courtImg.URL <- "https://thedatagame.files.wordpress.com/2016/03/nba_court.jpg"
court <- rasterGrob(readJPEG(getURLContent(courtImg.URL)),
           width=unit(1,"npc"), height=unit(1,"npc"))

train2<-train
closest<-hex_pos(train2$loc_x, train2$loc_y, 70,70) #create bins
train2$center_x=closest[,1] 
train2$center_y=closest[,2]

train2<-subset(train2, center_y<350 & abs(center_x)<250) #ignore backcourt shots. they are outliers

train3<-train2 %>%
  group_by(center_x, center_y) %>%
  summarise(makes=sum(shot_made_flag), tot=n()) %>%
  mutate(Accuracy=100*makes/tot) #get accuracy for each hexagon

ggplot(train3, aes(center_x,center_y, color=Accuracy)) + #create plot
  annotation_custom(court, -250, 250, -52, 418) +
  geom_point(aes(size=tot)) + 
  scale_color_gradient(low="blue", high="red") + 
  guides(alpha = FALSE, size = FALSE) +
  xlim(250, -250) +
  ylim(-52, 418) +
  xlab("") + ylab("") + ggtitle("Kobe Bryant Shot Accuracy")
```

Below, we have plotted the accuracy for each level of the `combined_shot_type` variable. Naturally, jump shots will be further from the basket than layups and dunks. This variable is unique from many of the other ones because it describes unique information about the defense and it also provides insight into the location of the shot. Naturally, layups and dunks will be within 5 feet or so of the basket, while jump shots will occur from much farther away. By knowing shot type, we know the general region a shot will fall into, making the distance and location variables less important. Additionally, we are also given information about the defense with the shot type variable. For instance, if a player is right next to the basket and there is no defense, he will dunk the ball. If there is very good defense, however, he may try a hook shot, a much more difficult shot. Even though there is not a variable directly referencing defense, `combined_shot_type` is unique in that it serves as a proxy for the defense, making it highly predictive of shot outcome.


```{r}
prop.table(table(train$combined_shot_type, train$shot_made_flag),1) -> temp
as.data.frame.matrix(temp) -> temp
temp$shot <- rownames(temp)
ggplot(temp, aes(x = reorder(shot, `1`), y = 1)) +
geom_point(aes(y = `1`), size = 3, color = " dark blue", stat = "identity") +
    coord_flip() +
    labs(y = "Accuracy", x = "", title = "Accuracy by Shot Type")
```



Note: If you had to illustrate using no modelling but only graphs and tables which
variables have the most predictive power, which would you include?

* Perform a cross-validation on only the final/ultimate model used for your
submission.
* The "score" in question should be the same as used to compute the Kaggle
leaderboard. In other words, your estimated score should be roughly equal to the
score returned by Kaggle after your submission.



## Crossvalidation of ultimate model

Note: Hardcode your crossvalidation here i.e. do not use built-in crossvalidation
options.

Below, we conduct cross-validation on our final logistic model with LASSO regularization. We determined our optimal $\lambda$ by conducting another cross-validation process. The cross-validation below is designed to give us an estimated Kaggle score. 

```{r, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}

 LogLossBinary = function(actual, predicted, eps = 1e-15) {
  predicted = pmin(pmax(predicted, eps), 1-eps)
  - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
} #https://www.r-bloggers.com/making-sense-of-logarithmic-loss/

set.seed(3)
lambda_inputs <- 10^seq(-2, 0, length = 100)

model_formula <- as.formula(shot_made_flag ~ combined_shot_type +
                              lat + lon + minutes_remaining +
                              period + playoffs + season + shot_distance + opponent)


# Predictor Matrix for Training Set
x <- model.matrix(model_formula, data = train)[, -1]
y <- train$shot_made_flag
# dummy filler so that we can create the test predictor matrix
test$shot_made_flag <- rep(c(1,0)) 

# Predictor Matrix for the Test Set
predictor_matrix_test <- model.matrix(model_formula, data = test)[,-1]
  
idx<-createFolds(y, k=10) #break data into 10 portions
errors<-rep(0, 10)
lambda_star=0.01668101 #determined through cross validation
for (i in 1:10) {
  psuedo_train=x[ -idx[[i]] , ]
  psuedo_test=x[ idx[[i]], ]
  outcomes<-y[ -idx[[i]] ]
  LASSO_fit <- glmnet(x=psuedo_train, y=outcomes, alpha = 1, 
                    lambda = lambda_star, family = "binomial")
  pred=predict(LASSO_fit, newx=psuedo_test)
  p_hat <- 1/(1 + exp(-pred))
   errors[i]=LogLossBinary(y[idx[[i]]], p_hat) #calculate loss
}
  mean(errors)
```

Our cross-validated score is nearly identical to our Kaggle Score of 0.65234.


## Create submission

Note: Output a CSV using `write_csv(DATAFRAME_NAME, path="data/SUBMISSION_NAME.csv")`
that is Kaggle submitable. This submission should return a Kaggle score that is
close to your crossvalidated score.


```{r}
LASSO_fit <- glmnet(x=x, y=y, alpha = 1, 
                    lambda = lambda_star, family = "binomial")
log_odds_hat <- predict(LASSO_fit, newx=predictor_matrix_test, s=lambda_star) %>% 
  as.vector()

p_hat <- 1/(1 + exp(-log_odds_hat))

test <- mutate(test, p_hat = p_hat)

test <- mutate(test, shot_made_flag = p_hat)

# create submission file
submission <- test[c(25, 15)]
write.csv(submission, "LASSO_submission.csv", row.names=FALSE)
```



## Citations and references

Note: All citations and references must be included here.

R documentation.
https://www.r-bloggers.com/making-sense-of-logarithmic-loss/


## Supplementary materials

Note: Anything else you've tried that you'd like to include, but isn't essential to
the above, like other EDA's, other modeling approaches you've tried, etc. Please
set the R code chunk `eval=FALSE` here so that default is that R Markdown
doesn't run the code, but a user can flip this switch if they are curious.


The code below is the code we used for the cross-validation process in determining the optimal value for K in our K-NN model. We ultimately fit a K-NN model with $K=250$, but it was not as successful as our logistic model.

```{r, eval=FALSE, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5, echo=FALSE}
set.seed(3)
k_vals=rep(0,99)
k=5


for (i in 1:length(k_vals)){
  k_vals[i]=k
  k=k+5
}
y=train$shot_made_flag #outcome
x <- train[, c(5, 8, 10, 12)] %>% #predictors
  mutate(season=strtrim(season, 4)) %>%
  mutate(season=as.numeric(season))

  
idx<-createFolds(y, k=10) #break data into 10 portions
errors_for_k=rep(0, length(k_vals))
for (j in 1:length(k_vals)){
  errors<-rep(0, 10)
for (i in 1:10) {
  psuedo_train=x[ -idx[[i]] , ]
  psuedo_test=x[ idx[[i]], ]
  outcomes<-y[ -idx[[i]] ]
  pred <- attr(class::knn(train=psuedo_train, test=psuedo_test, cl=outcomes, 
                          k=k_vals[j], prob=TRUE), "prob") 
  #build model on 9 folds and predict on other fold
   errors[i]=LogLossBinary(y[idx[[i]]], pred) #calculate loss
}
  errors_for_k[j]=mean(errors)
}

cross<-data.frame(k=k_vals, cv_mse=errors_for_k)

ggplot(filter(cross, k>50), aes(k, cv_mse))+geom_point() +
  ggtitle("Cross Validated Log Loss") +
  xlab("K") + ylab("Log Loss")


```





